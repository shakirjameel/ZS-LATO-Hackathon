{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shakirjameel/ZS-LATO-Hackathon/blob/main/LATO_Hackathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nothing Fancy Here!**"
      ],
      "metadata": {
        "id": "GYIFaSY8pwI8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RdCaxX1kANTY"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U --quiet langchain-community tiktoken langchain-openai langchainhub chromadb langchain langgraph langchain-text-splitters\n",
        "%pip install --upgrade --quiet faiss-cpu\n",
        "%pip install --upgrade --quiet pypdf\n",
        "%pip install --upgrade --quiet rapidocr-onnxruntime\n",
        "%pip install langchain_chroma langchain_openai\n",
        "%pip install arxiv\n",
        "%pip install autogen\n",
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import json\n",
        "import re\n",
        "from typing import Annotated\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import autogen\n",
        "from autogen import AssistantAgent, UserProxyAgent\n",
        "from autogen import ConversableAgent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAlF5e0mXobz",
        "outputId": "d5c03fec-3637-49a7-81d6-d909643fceab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autogen in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from autogen) (5.6.3)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.10/dist-packages (from autogen) (7.1.0)\n",
            "Requirement already satisfied: flaml in /usr/local/lib/python3.10/dist-packages (from autogen) (2.3.1)\n",
            "Requirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from autogen) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.3 in /usr/local/lib/python3.10/dist-packages (from autogen) (1.48.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from autogen) (24.1)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogen) (2.9.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from autogen) (1.0.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from autogen) (2.4.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from autogen) (0.7.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3->autogen) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen) (2.23.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen) (2.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->autogen) (2024.9.11)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->autogen) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->autogen) (3.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add your open AI key in this cell**"
      ],
      "metadata": {
        "id": "kZYXWvHEp7lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"<ENTER KEY HERE>\"\n",
        "llm_config = {\"model\": \"gpt-4\", \"api_key\": api_key}\n",
        "# assistant = AssistantAgent(\"assistant\", llm_config=llm_config)"
      ],
      "metadata": {
        "id": "8emwxajDiRJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Everything related to fetching reasearch papers from arXiv**"
      ],
      "metadata": {
        "id": "VA6E2gt6qFb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_page(page_content, page_number, article_title):\n",
        "    \"\"\"\n",
        "    Sends the page content to OpenAI for summarization, specifying the page number\n",
        "    and that other instances are summarizing the remaining pages.\n",
        "\n",
        "    Parameters:\n",
        "    - api_key (str): Your OpenAI API key.\n",
        "    - page_content (str): The text content of the page to be summarized.\n",
        "    - page_number (int): The page number being summarized.\n",
        "    - article_title (str): The title of the arXiv article.\n",
        "\n",
        "    Returns:\n",
        "    - str: The summary returned by OpenAI.\n",
        "    \"\"\"\n",
        "    print(f\"Extracting abstract in page {page_number} of article {article_title}\")\n",
        "    client = OpenAI(\n",
        "        # This is the default and can be omitted\n",
        "        api_key=api_key,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Construct the prompt\n",
        "    prompt = (f\"This is page {page_number} from an article titled '{article_title}' from arXiv. \"\n",
        "              f\"This page of the paper contains the abstract and maybe more content. Please isolate the abstract of this article\"\n",
        "              f\"Here is the content of page {page_number}:\\n\\n{page_content}\\n\\n\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=\"gpt-4\",\n",
        "    )\n",
        "\n",
        "    # Extract and return the summary\n",
        "    summary = response.choices[0].message.content\n",
        "    return summary\n",
        "\n",
        "def remove_invalid_characters(input_string, pattern):\n",
        "    # Removes characters from the input_string that do not match the given regex pattern.\n",
        "    # Create a regex pattern to match all characters that are NOT in the given pattern\n",
        "    invalid_pattern = f'[^{pattern}]'\n",
        "\n",
        "    # Replace all invalid characters with an empty string\n",
        "    cleaned_string = re.sub(invalid_pattern, '', input_string)\n",
        "\n",
        "    return cleaned_string\n",
        "\n",
        "def get_title_from_url(url):\n",
        "    # Extract the arXiv ID from the URL\n",
        "    arxiv_id = url.split('/')[-1]  # Extract the arXiv ID from the URL\n",
        "\n",
        "    # Construct the default API client\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # Create a search query for the specific arXiv ID\n",
        "    search = arxiv.Search(id_list=[arxiv_id])\n",
        "\n",
        "    # Get the search result\n",
        "    results = client.results(search)\n",
        "\n",
        "    # Fetch the first result, since ID searches are unique\n",
        "    paper = next(results, None)\n",
        "    return paper.title\n",
        "\n",
        "def extract_text_from_docs(docs_list):\n",
        "    \"\"\"Extract and concatenate the text from a list of lists of Documents.\"\"\"\n",
        "    new_doc_list = []\n",
        "    for docs in docs_list:\n",
        "        full_summary = \"\"\n",
        "        name = get_title_from_url(docs[0].metadata['source'])\n",
        "        name = remove_invalid_characters(name, '^[a-zA-Z0-9_-]+$')\n",
        "        for doc in docs:\n",
        "            if 'abstract' not in doc.page_content.lower():\n",
        "              continue\n",
        "            abstract = doc.page_content.lower().split('abstract')[1]\n",
        "            summary = summarize_page(doc.page_content, doc.metadata['page'], name)\n",
        "            summary += \"\\n\"  # Add a newline to separate pages/documents\n",
        "            full_summary += summary\n",
        "            break\n",
        "        full_summary = remove_invalid_characters(full_summary, '^[a-zA-Z0-9_-]+$')\n",
        "        doc_data = {\"article_name\": name, \"article_content\": full_summary}\n",
        "        new_doc_list.append(doc_data)\n",
        "    return new_doc_list\n",
        "\n",
        "def load_pdfs(urls):\n",
        "    print(\"Loading arXiv Research Papers ...\")\n",
        "    docs_list = []\n",
        "    for url in urls:\n",
        "        loader = PyPDFLoader(url, extract_images=True)\n",
        "        docs = loader.load()\n",
        "        docs_list.append(docs)\n",
        "    docs_list = extract_text_from_docs(docs_list)\n",
        "    return docs_list\n",
        "\n",
        "# [{\"article_name\":\"name\", \"article_content\": \"content\"}]\n",
        "def search_papers(query):\n",
        "  # Construct the default API client.\n",
        "  client = arxiv.Client()\n",
        "  # Search for the 2 most recent articles matching the query\n",
        "  print(\"Searching arXiV Papers ...\")\n",
        "  search = arxiv.Search(\n",
        "    query = query,\n",
        "    max_results = 2,\n",
        "    sort_by = arxiv.SortCriterion.Relevance\n",
        "  )\n",
        "  results = client.results(search)\n",
        "  papers_dict = {}\n",
        "  for paper in results:\n",
        "    papers_dict[paper.title] = paper.pdf_url\n",
        "  papers_urls = papers_dict.values()\n",
        "  docs = load_pdfs(papers_urls)\n",
        "  print(\"Ready for summarization!\")\n",
        "  return json.dumps(docs)"
      ],
      "metadata": {
        "id": "MEqgwIICB4oV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docs = search_papers(\"llms for cooking\")\n",
        "# print(docs)"
      ],
      "metadata": {
        "id": "bCrshZY9AOyZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[WIP] This still needs work**"
      ],
      "metadata": {
        "id": "i2WCaI4vqNUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "associate = \"\"\"\n",
        "Statement:Has skills to participate in the problem solving process for structured problems\n",
        "Additional Statements:\n",
        "Understands how different problem solving approaches could provide insight\n",
        "Creates and executes work in a careful and comprehensive way (code, test scripts, plans, SOPs; Runbook, etc.)\n",
        "Statement:Able to implement technical specifications developed by others\n",
        "Additional Statements:\n",
        "Understands business requirements and specifications and implements them per technical/functional specifications or creates requirements documentation, test cases or scenarios\n",
        "Correctly understands and implements technical requirements, and\n",
        "application architectures that were\n",
        "designed by others\n",
        "Under supervision, can develop artifacts such as wireframe diagrams, demo sites,click-through demos, etc\n",
        "Statement:Uses existing tools and group knowledge\n",
        "Additional Statements:\n",
        "Learns about tools, reusable components, and infrastructure to deliver results on the project\n",
        "Understands and uses best practices and standards (business process, design, QA, test, application support)\n",
        "Understands which tools and processes to use to execute role and resolve common subject area issues and problems that arise\n",
        "Statement:Develops an understanding of the client organization and industry\n",
        "Additional Statements:\n",
        "Learns about client’s products, markets, competitors and key issues through interaction with the project team and additional research\n",
        "Learns and applies knowledge about client’s products, markets,competitors and key issues through interaction with the project team and additional research\n",
        "Develops an understanding of key trends in the client’s industry\n",
        "Can work effectively with databases, SQL, and other core infrastructure elements, as relevant\n",
        "Statement:Demonstrates the aptitude to develop and apply expertise\n",
        "Additional Statements:\n",
        "Demonstrates adaptability to evolve expertise by learning new areas of technologies, domain & delivery methodology\n",
        "Contributes ideas to improve or develop new techniques or tools\n",
        "Demonstrates intellectual curiosity in approach to executing analyses\n",
        "Applies rigorous design and coding practices on projects\n",
        "\"\"\"\n",
        "\n",
        "consultant = \"\"\"\n",
        "Statement:Gains credibility through knowledge of topic area and confident presentation of content\n",
        "Additional Statements:\n",
        "Presents a compelling viewpoint to clients/team, supported by input from the project manager or Principal\n",
        "Interacts comfortably and effectively with the project contact and individuals supporting him/her\n",
        "Keeps the audience focused on the objective and minimizes divergences\n",
        "Statement:Shapes and delivers well structured and compelling written materials to support project, team and client communication\n",
        "Additional Statements:\n",
        "Writes client-facing or client-directed communication to appropriate audience that emphasizes key messages and links project work to relevant client business issues\n",
        "Structures written communication to ensure clear direction regarding team member roles, action items, key milestones, methodology decisions, etc.\n",
        "Provides guidance and coaching to team members regarding appropriate written communication objectives / message, audience, channel, tone, etc.\n",
        "Helps define and document what good written deliverables look like\n",
        "Develops documentation/diagrams to systematically convey system architecture to clients (where relevant)\n",
        "Develops well-written effective & visually compelling storyboards and materials (e.g. executive summaries, deck that convey appropriate structure, articulate key points, and guide team in execution of research, analysis, synthesis)\n",
        "Structures communication effectively in a logical manner following best practice guidelines (e.g., Pyramid Principle)\n",
        "Communicates complex information in a meaningful, easy to understand way\n",
        "Synthesizes project findings into key insights /recommendations\n",
        "Provides appropriate backup materials to support points and recommendations\n",
        "Statement:Solves structured problems\n",
        "Additional Statements:\n",
        "Conceptualizes the client or technical issue and works variously with the solution architect to develop approach; makes good testing choices to cover scenarios and functionality\n",
        "Leverages previous ZS projects\n",
        "Is comfortable with ambiguity\n",
        "Commits to identifying solutions and does not make premature judgments about what will and will not work\n",
        "Statement: Applies problem solving frameworks\n",
        "Additional Statements:\n",
        "Works within frameworks that the client is comfortable with\n",
        "Has the business, operational and technical skills/credibility to specify solution or approach for the team\n",
        "Has technical skills required to lead the project team in domain area\n",
        "Knows when and where to prioritize efforts in design, model, testing or infrastructure for greatest value\n",
        "Statement: Has a broad understanding of technology trends, key issues and challenges in the project\n",
        "Additional Statements:\n",
        "Understands how trends impact client organization(s)\n",
        "Probes the client or information to obtain a full understanding of issues\n",
        "Could have a issue-based technology discussion with a client, if prepared\n",
        "Statement: Demonstrates expertise in a way that enhances project results\n",
        "Additional Statements:\n",
        "Demonstrates adaptability to evolve expertise by learning new areas of technologies\n",
        "Is an expert in at least few technology/ tool areas and can resolve technofunctional issues\n",
        "Advises and influences project design, solutions. Technology/ tools\n",
        "Improves skills and knowledge of team members in area of technology expertise\n",
        "Come up with technology solutions that are in-line with organization technology road map\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GYs_ELoVL1nF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Registering custom function to the agent**"
      ],
      "metadata": {
        "id": "H0Pa3uRvqSHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_papers(\n",
        "    query: Annotated[str, \"Query string to fetch papers.\"]\n",
        ") -> str:\n",
        "    str_arr_of_papers = search_papers(query)\n",
        "    return str_arr_of_papers"
      ],
      "metadata": {
        "id": "fE24rllupl3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent for fetching information and summarization**"
      ],
      "metadata": {
        "id": "Ihpe7wSJqaqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article_fetcher = ConversableAgent(\n",
        "    name=\"article_fetcher\",\n",
        "    system_message=\"You are a helpful AI assistant. \"\n",
        "    \"Roles can be of the type associate, and a consultant.\"\n",
        "    \"If you cant find any of the roles in the question, ask the user for the role and mention that if the user has already shared the role, highlight that you can only summarize articles for role mentioned in this prompt.\"\n",
        "    \"You can help with fetching articles by creating the query based on user input and passing it to the registered function fetch papers.\"\n",
        "    f\"If the role is associate use this as an additional context for your summary: {associate}\"\n",
        "    f\"If the role is consultant use this as an additional context for your summary: {consultant}\"\n",
        "    \"Return the article_content as a detailed summary\"\n",
        "    # \"Return 'TERMINATE' when the task is done.\"\n",
        "    ,\n",
        "    human_input_mode=\"ALWAYS\",\n",
        "    llm_config={\"config_list\": [llm_config]},\n",
        ")\n",
        "article_fetcher.register_for_llm(name=\"fetch_papers\", description=\"arXiV paper fetcher\")(fetch_papers)\n"
      ],
      "metadata": {
        "id": "iREbtWqOs7vT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "5ef8833f-22ac-4933-8200-46e9edb359f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.fetch_papers(query: typing.Annotated[str, 'Query string to fetch papers.']) -> str>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>fetch_papers</b><br/>def fetch_papers(query: Annotated[str, &#x27;Query string to fetch papers.&#x27;]) -&gt; str</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-15-2c8aea03df2c&gt;</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**User Proxy Agent**"
      ],
      "metadata": {
        "id": "ff-I_8iqqqc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The user proxy agent is used for interacting with the assistant agent\n",
        "# and executes tool calls.\n",
        "user_proxy = ConversableAgent(\n",
        "    name=\"User\",\n",
        "    llm_config=False,\n",
        "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "user_proxy.register_for_execution(name=\"fetch_papers\")(fetch_papers)"
      ],
      "metadata": {
        "id": "cEinEIP-qo1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from autogen import register_function\n",
        "\n",
        "# # Register the calculator function to the two agents.\n",
        "# register_function(\n",
        "#     fetch_papers,\n",
        "#     caller=article_fetcher,  # The assistant agent can suggest calls to the calculator.\n",
        "#     executor=user_proxy,  # The user proxy agent can execute the calculator calls.\n",
        "#     name=\"fetch_papers\",  # By default, the function name is used as the tool name.\n",
        "#     description=\"arXiV paper fetcher\",  # A description of the tool.\n",
        "# )"
      ],
      "metadata": {
        "id": "ueL6uaf0urCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This runs the agentic flow**"
      ],
      "metadata": {
        "id": "CBjUYm-mqxfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_result = user_proxy.initiate_chat(article_fetcher, message=\"Give me the latest Gen AI Articles? summarize it for an associate\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E1QdcK_vE2J",
        "outputId": "016d823e-1490-431c-b242-65098ccada1d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User (to article_fetcher):\n",
            "\n",
            "Give me the latest Gen AI Articles? summarize it for an associate\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as article_fetcher. Provide feedback to User. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "article_fetcher (to User):\n",
            "\n",
            "***** Suggested tool call (call_fzQ3OtRoauzpXgv4fcK7NiIv): fetch_papers *****\n",
            "Arguments: \n",
            "{\n",
            "  \"query\": \"Latest General AI Articles\"\n",
            "}\n",
            "*****************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION fetch_papers...\n",
            "searching...\n",
            "Loading papers...\n",
            "extracting abstract in page 0 of article On the Combination of AI and Wireless Technologies: 3GPP Standardization Progress\n",
            "extracting abstract in page 0 of article Landscape of Generative AI in Global News: Topics, Sentiments, and Spatiotemporal Analysis\n",
            "Ready\n",
            "User (to article_fetcher):\n",
            "\n",
            "User (to article_fetcher):\n",
            "\n",
            "***** Response from calling tool (call_fzQ3OtRoauzpXgv4fcK7NiIv) *****\n",
            "[{\"article_name\": \"On the Combination of AI and Wireless Technologies: 3GPP Standardization Progress\", \"article_content\": \"Abstract \\u2014Combing Artificial Intelligence (AI) and wireless communication technologies has become one of the major technologies trends towards 2030. This includes using AI to improve the efficiency of the wireless transmission and supporting AI deployment with wireless networks. In this article, the latest progress of the Third Generation Partnership Project (3GPP) standards development is introduced. Concentrating on AI model distributed transfer and AI for Beam Management (BM) with wireless network, we introduce the latest studies and explain how the existing standards should be modified to incorporate the results from academia.\\n\"}, {\"article_name\": \"Landscape of Generative AI in Global News: Topics, Sentiments, and Spatiotemporal Analysis\", \"article_content\": \"The Abstract of the article 'Landscape of Generative AI in Global News: Topics, Sentiments, and Spatiotemporal Analysis' is:\\n\\nGenerative AI has exhibited considerable potential to transform various industries and public life. The role of news media coverage of generative AI is pivotal in shaping public perceptions and judgments about this significant technological innovation. This paper provides in-depth analysis and rich insights into the temporal and spatial distribution of topics, sentiment, and substantive themes within global news coverage focusing on the latest emerging technology \\u2013generative AI. We collected a comprehensive dataset of news articles (January 2018 to November 2023, N= 24,827). For topic modeling, we employed the BERTopic technique and combined it with qualitative coding to identify semantic themes. Subsequently, sentiment analysis was conducted using the RoBERTa-base model. Analysis of temporal patterns in the data reveals notable variability in coverage across key topics\\u2013business, corporate technological development, regulation and security, and education\\u2013with spikes in articles coinciding with major AI developments and policy discussions. Sentiment analysis shows a predominantly neutral to positive media stance, with the business-related articles exhibiting more positive sentiment, while regulation and security articles receive a reserved, neutral to negative sentiment. Our study offers a valuable framework to investigate global news discourse and evaluate news attitudes and themes related to emerging technologies.\\n\"}]\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as article_fetcher. Provide feedback to User. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "article_fetcher (to User):\n",
            "\n",
            "Sure, here are summaries of the latest articles on General AI:\n",
            "\n",
            "1. **On the Combination of AI and Wireless Technologies: 3GPP Standardization Progress**\n",
            "This article discusses the integration of Artificial Intelligence (AI) and wireless communication technologies, which is becoming a key technology trend for the future. Specifically, it focuses on using AI to enhance the efficacy of wireless transmission and to aid AI deployment within wireless networks. It provides details on the advancement of the Third Generation Partnership Project (3GPP) standards. Emphasizing AI model distributed transfer and AI for Beam Management (BM) with wireless networks, the article narrates the latest research and deliberates the modifications needed in the existing standards to consolidate academic results.\n",
            "\n",
            "   *For an associate*, along with problem-solving skills, this involves comprehending how diverse problem-solving approaches can provide insights. It will require careful execution of work and understanding how to utilize existing tools and group knowledge to achieve results. Additionally, presence of client understanding, aptitude to develop and apply expertise while administering technical specifications can prove beneficial.\n",
            "\n",
            "2. **Landscape of Generative AI in Global News: Topics, Sentiments, and Spatiotemporal Analysis**\n",
            "This paper analyzes the temporal and spatial distribution of topics, sentiment, and substantive themes within global news coverage with an emphasis on Generative AI. It collected data from news articles over a span of five years (January 2018 to November 2023) and applied the BERTopic technique combined with qualitative coding for topic modeling. Sentiment analysis was conducted using the RoBERTa-base model, and it was found that business-related articles exhibited more positive sentiment, while regulation and security articles received a neutral to negative sentiment.\n",
            "\n",
            "   *For an associate*, this article can assist in developing a clear understanding of recent trends in AI and how they are being received globally. It also provides an opportunity to expand one's skillsets by learning and applying new technologies or tools such as BERTopic and RoBERTa-base model. It's crucial to understand how these trends can impact the client organization and provide detailed solutions accordingly.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "User (to article_fetcher):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as article_fetcher. Provide feedback to User. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3KVpJdeBPon-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SCRATCH CODE, PLEASE IGNORE**"
      ],
      "metadata": {
        "id": "Etg1St-rq23Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "research_paper_fetcher = AssistantAgent(\n",
        "    name=\"research_paper_fetcher\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    system_message=\n",
        "    \"\"\"Assistant to fetch papers from the registered funtion fetch_papers and summarize the articles based on the role mentioned in the question.\n",
        "    Roles can be of the type associate, and a consultant.\n",
        "    If you cant find any of the roles in the question, ask the user for the role and mention that if the user has already shared the role, highlight that you can only summarize articles for role mentioned in this prompt.\n",
        "    While returning the results, use the format -\n",
        "    Original Articles: Name of the article in bold followed by the orignal content returned by the registered function fetch_papers in a separate line\n",
        "    Article Summary: Name of the article in bold follwoed by the summary of the article\n",
        "    Reply TERMINATE when the task is done.\"\"\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "-xxFKhNQclPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autogen.agentchat.register_function(\n",
        "    fetch_papers,\n",
        "    caller=research_paper_fetcher,\n",
        "    executor=user_proxy,\n",
        "    name=\"fetch_papers\",\n",
        "    description=\"Function to fetch data from arXiv\",\n",
        ")"
      ],
      "metadata": {
        "id": "WUjuUFomY36M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30111662-13d4-43ba-9f70-374f62431d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py:2492: UserWarning: Function 'fetch_papers' is being overridden.\n",
            "  warnings.warn(f\"Function '{name}' is being overridden.\", UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy.initiate_chat(research_paper_fetcher, message = \"Gen AI Articles for a consultant\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "U-z3Nwh5jMb0",
        "outputId": "4edb5fca-eda6-43fc-dd5d-1efc4853c88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to research_paper_fetcher):\n",
            "\n",
            "Gen AI Articles for a consultant\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "research_paper_fetcher (to user_proxy):\n",
            "\n",
            "Sure. Let me fetch the articles for the role \"consultant\" and summarize them.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as user_proxy. Provide feedback to research_paper_fetcher. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \n",
            "\n",
            ">>>>>>>> NO HUMAN INPUT RECEIVED.\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "user_proxy (to research_paper_fetcher):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "research_paper_fetcher (to user_proxy):\n",
            "\n",
            "I am sorry but I seem to have encountered a problem in fetching the articles. Could you please try again later?\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-a98281273237>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_proxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitiate_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresearch_paper_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Gen AI Articles for a consultant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36minitiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0mmsg2send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_init_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg2send\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         summary = self._summarize_chat(\n\u001b[1;32m   1104\u001b[0m             \u001b[0msummary_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     async def a_receive(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_oai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_sending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_reply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mreceive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_reply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_reply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreply_at_receive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mgenerate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   2054\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply_func_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trigger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m                 \u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreply_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreply_func_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlogging_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m                     log_event(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mcheck_termination_and_human_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1806\u001b[0m         \u001b[0msender_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"the sender\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msender\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_input_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ALWAYS\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1808\u001b[0;31m             reply = self.get_human_input(\n\u001b[0m\u001b[1;32m   1809\u001b[0m                 \u001b[0;34mf\"Replying as {self.name}. Provide feedback to {sender_name}. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/agentchat/conversable_agent.py\u001b[0m in \u001b[0;36mget_human_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   2180\u001b[0m         \u001b[0miostream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIOStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2182\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miostream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_human_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/autogen/io/console.py\u001b[0m in \u001b[0;36minput\u001b[0;34m(self, prompt, password)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"Password: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the latest Gen AI papers for a principal/ consultant\n",
        "# User Proxy -> Paper Fetcher -> Summarizer (Regex Magic) -> Result"
      ],
      "metadata": {
        "id": "wQN3cotbcfIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarizer = AssistantAgent(\n",
        "#     name=\"summarizer\",\n",
        "#     llm_config=llm_config,\n",
        "#     system_message = \"\"\"\n",
        "#         You are a professional writer of summaries of articles, known for\n",
        "#         your insightful and engaging summaries.\n",
        "#         You transform complex concepts into compelling narratives.\n",
        "#         Reply \"TERMINATE\" in the end when everything is done.\n",
        "#     \"\"\",\n",
        "# )"
      ],
      "metadata": {
        "id": "mGf7yfr0e15I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat_results = user_proxy.initiate_chats(\n",
        "#     [\n",
        "#         {\n",
        "#             \"recipient\": research_paper_fetcher,\n",
        "#             \"message\": summary_tasks[0],\n",
        "#             \"clear_history\": True,\n",
        "#             \"silent\": False,\n",
        "#             \"summary_method\": \"last_msg\",\n",
        "#         },\n",
        "#         {\n",
        "#             \"recipient\": summarizer,\n",
        "#             \"message\": summary_tasks[1],\n",
        "#             \"summary_method\": \"reflection_with_llm\",\n",
        "#         },\n",
        "#         {\n",
        "#             \"recipient\": writer,\n",
        "#             \"message\": writing_tasks[0],\n",
        "#             \"carryover\": \"I want to include a figure or a table of data in the blogpost.\",\n",
        "#         },\n",
        "#     ]\n",
        "# )"
      ],
      "metadata": {
        "id": "Sy2tOohjgBZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary_tasks = [\n",
        "#     \"\"\"What are the latest articles in Gen AI?\"\"\",\n",
        "#     \"\"\"Summarize the content for a consultant\"\"\"\n",
        "\n",
        "# ]\n",
        "\n",
        "# writing_task = [\n",
        "#     \"\"\"Develop a personalized summary of the articles which are relavant to the competancy of the role mentioned\"\"\"\n",
        "# ]"
      ],
      "metadata": {
        "id": "HQgsE-QDgewG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat_results = user_proxy.initiate_chats(\n",
        "#     [\n",
        "#         {\n",
        "#             \"recipient\": research_paper_fetcher,\n",
        "#             \"message\": summary_tasks[0],\n",
        "#         },\n",
        "#         {\n",
        "#             \"recipient\": summarizer,\n",
        "#             \"message\": summary_tasks[1],\n",
        "#             \"summary_method\": \"reflection_with_llm\",\n",
        "#         },\n",
        "#         {\n",
        "#             \"recipient\": writer,\n",
        "#             \"message\": writing_tasks[0],\n",
        "#             \"carryover\": \"I want to include a figure or a table of data in the blogpost.\",\n",
        "#         },\n",
        "#     ]\n",
        "# )"
      ],
      "metadata": {
        "id": "vG-f2xH43tQ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}